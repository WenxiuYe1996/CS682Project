
<!DOCTYPE html>

<html lang="python">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Clien&#39;s Concern: &#8212; word salad ... documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="clien-s-concern">
<h1>Clien's Concern:<a class="headerlink" href="#clien-s-concern" title="Permalink to this headline">¶</a></h1>
<p>With the ever evolving space that is climate change, there is a vast amount of data that is used across disciplines.
Communicating climate change information has become increasingly more difficult as reliable baselines and equivalencies are scarce.
NGOs, corporations and individuals should have the opportunity to have access to reliable climate change data and equivalencies in order to better understand this interconnective space.
As the volume of technical and scientific publications increase across academia, industry, and governmental sources, the task of collecting relevant data on a particular topic of interest becomes increasingly onerous.
Machine learning models leveraging natural language processing techniques offer an opportunity to extract meaningful information with semantic similarities rather than simple spelling.
Visualizations of an embedded space of results could also be useful in determining outliers making spurious claims.</p>
</section>
<section id="project-implementation">
<h1>Project implementation:<a class="headerlink" href="#project-implementation" title="Permalink to this headline">¶</a></h1>
<p>To determine if the content of a document is reliable or not, one approach to it is to compare it with other documents with a similar topic.
In order to accomplish this, we need to collect data from search engines and use NLP tools to process it.
NLP(natural language processing) refers to a computer program that has the ability to understand text and spoken words in the same way as humans.
I have written a web scraper that will perform a serch on NASA’s website according to user input topic and download the documents in pdf format.
Once all the documents are downloaded, the code will convert them into a text file. To extract the topic from the texts, we can use a machine learnning model called LDA.
Latent Dirichlet Allocation (LDA) which is an unsupervised learning algorithm which finds the underlying topic of a document.
The topic generated by LDA model is a collection of keywords.
Just by looking at the keywords, you can identify what the topic is all about.
Before we can build the LDA model, we have to do some data cleaning.
For example, we have to make all letters lowercase, remove punctuations, numbers, empty spaces and references if there’s any in the documents.
We then split the document into sentences, sentences into words and then we lemmatize the words.
Next we build the LDA model using python gensim library.
Once the process is completed, we can get the underlying topic of the given documents.
The topic of each document is a collection of keywords with weight.
So the API will read all the keywords and try to find keywords that match the user’s input topic.
Then we sort the documents in descending order by their weight.
The documents with weight zero will be discard as it proves not  to be related to what user is looking for.
For the remaindng documents, the code will group them by their topic.
Documents with similar topics will have similar content semantically.
This can be useful info for the user to determine if the document is reliable or not.
We calcaluate the similarities between each of them using doc2vec model.
Doc2vec is an NLP tool for representing documents as a vector and is a generalizing of the word2vec method.</p>
</section>
<section id="usage">
<h1>Usage<a class="headerlink" href="#usage" title="Permalink to this headline">¶</a></h1>
<section id="installation">
<h2>Installation<a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h2>
<p>To use this API, please install python3 and the requied libraries.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp gp-VirtualEnv">(.venv)</span> <span class="gp">$ </span>pip3 install -r requirements.txt
</pre></div>
</div>
</section>
<section id="to-run-the-api">
<h2>To run the API<a class="headerlink" href="#to-run-the-api" title="Permalink to this headline">¶</a></h2>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp gp-VirtualEnv">(.venv)</span> <span class="gp">$ </span><span class="nb">cd</span> api
<span class="gp gp-VirtualEnv">(.venv)</span> <span class="gp">$ </span>python3 app.py
</pre></div>
</div>
</section>
</section>
<section id="endpoints-for-this-api">
<h1>Endpoints for this API<a class="headerlink" href="#endpoints-for-this-api" title="Permalink to this headline">¶</a></h1>
<section id="endpoint-search">
<h2>Endpoint: /search<a class="headerlink" href="#endpoint-search" title="Permalink to this headline">¶</a></h2>
<p>To download the files from NASA website, use the /search endpoint with a given topic.
This endpoint will search the NASA's website based on the user's input topic and download all the files found.</p>
<p><code class="docutils literal notranslate"><span class="pre">search(userInputTopic,</span> <span class="pre">lang=&quot;en&quot;):</span></code> function:</p>
<blockquote>
<div><p>This funciton will perform a search on NASA's website and all files found will be downloaded.
Once all the files are downloaded, a csv file with three columns will be generated as well as a HTML verison of it; <br>
the first column contain the link to the article where it is found. <br>
the second column contain the link to the article description. <br>
the third column contain the local path to the article. <br></p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="search">
<span class="sig-name descname"><span class="pre">search</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#search" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to the generated csv file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputTopic</strong> -- user input topic.</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A path to the generated csv file and a path to the generated HTML file</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list[str]</p>
</dd>
</dl>
</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">makeNewDiretoryForGivenTopic(userInputTopic):</span></code> function:</p>
<blockquote>
<div><p>This function will create directories for user input topic to store the data.|br|
If user types in climate change, then the main directory climatechange will be created along with six subdirectories;
pdfclimatechange, paperclimatechange, txtclimatechange, csvclimatechange, imageclimatechange, ldaclimatechange <br></p>
<p>pdfclimatechange : used to store downloaded files <br>
paperclimatechange : used to store the identified papers for given user input topic <br>
txtclimatechange : used to store text extracted from papers <br>
csvclimatechange : used to store the paths to datas <br>
imageclimatechange : used to store the images extracted from the papers <br>
ldaclimatechange : used to store the lda models for papers</p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="makeNewDiretoryForGivenTopic">
<span class="sig-name descname"><span class="pre">makeNewDiretoryForGivenTopic</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#makeNewDiretoryForGivenTopic" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the directories names.(pdfDirName, paperDirName, imageDirName, csvDirName, txtDirName, ldaDirName)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputTopic</strong> -- user input topic.</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The directory names for given user input topic</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str, str, str, str, str, str</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="endpoint-extractinfofrompapers">
<h2>Endpoint: /extractInfoFromPapers<a class="headerlink" href="#endpoint-extractinfofrompapers" title="Permalink to this headline">¶</a></h2>
<p>To extract the text and the images from the papers you can use the /extractInfoFromPapers endpoint</p>
<p><code class="docutils literal notranslate"><span class="pre">extractInfoFromPapers(userInputTopic):</span></code> function:</p>
<blockquote>
<div><p>This funciton will search for a directory name pdf{userInputTopic}(without spaces) and retrieve all the files stored there.
Once all the files are obtained; <br>
-It will identify if the file is a paper or not by searching for abstract and refenereces in the article</p>
<p>If the file is a paper, then it will extract text from it and save the text under the text{userInputTopic}(without spaces) directory.
And it will extract images from it and save the images as one pdf file under the image{userInputTopic}(without spaces) directory.</p>
<p>Finally, a csv file with three columns will be generated as well as the HTML verison of it. <br>
The first column will contain the path to the papers. <br>
The second column will contain the path to the text file corresponding to the pappers. <br>
The last column will contain the path to the image file. <br></p>
<p>In order to save this table, a CSV file will be generated.</p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="extractInfoFromPapers">
<span class="sig-name descname"><span class="pre">extractInfoFromPapers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#extractInfoFromPapers" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to the generated csv file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputTopic</strong> -- user input topic.</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A path to the generated csv file and A path to the generated HTML file</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list[str]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="endpoint-findtopicforpapers">
<h2>Endpoint: /findTopicForPapers<a class="headerlink" href="#endpoint-findtopicforpapers" title="Permalink to this headline">¶</a></h2>
<p>To find topic of all the pappers you can use the /findTopicForPapers endpoint</p>
<p><code class="docutils literal notranslate"><span class="pre">findTopicForPapers(userInputTopic):</span></code> function:</p>
<blockquote>
<div><p>This function will search for the csv file generated by the /extractInfoFromPapers endpoint.
It will retrieve
the path to all the papers,
the path to the text file,
the path to the images. Then it will go through all the text file individually and find the underlying topic of the text using LDA topic model. <br>
The topic of each document is a collection of keywords with weight. Once the topic for each topic is identified,
it will read all the keywords and try to find keywords that match the user’s input topic.
If there is a match, then we keep the paper and classify it as document that related to what the user is looking for.
Then the documents will be sorted in descending order by their weight.</p>
<p>Finally, a csv file with six columns will be generated as well as the HTML verison of it. <br>
The first column will contain the path to the papers. <br>
The second column will contain the path to the text file corresponding to the papers. <br>
The third column will contain the LDA topic key wordlist. <br>
The fourth column will contain the path to the LDA model. <br>
The fifth column will contain weight. <br>
The sixth column will contain the path to the image file. <br></p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="findTopicForPapers">
<span class="sig-name descname"><span class="pre">findTopicForPapers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#findTopicForPapers" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to the generated csv file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputTopic</strong> -- user input topic.</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A path to the generated csv file and A path to the generated HTML file</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list[str]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="endpoint-findsimilarityforpapers">
<h2>Endpoint: /findSimilarityForPapers<a class="headerlink" href="#endpoint-findsimilarityforpapers" title="Permalink to this headline">¶</a></h2>
<p>To find topic of all the pappers you can use the /findTopicForPapers endpoint</p>
<p><code class="docutils literal notranslate"><span class="pre">findSimilarityForPapers(userInputTopic):</span></code> function:</p>
<blockquote>
<div><p>This function will search for the csv file generated by the /extractInfoFromPapers endpoint.
It will retrieve the path to all the papers, the path to the text file, the path to the images.
Then it will go through all the text file individually and find the underlying topic of the text using LDA topic model.
The topic of each document is a collection of keywords with weight. Once the topic for each topic is identified,
it will read all the keywords and try to find keywords that match the user’s input topic.
If there is a match, then we keep the paper and classify it as document that related to what the user is looking for.
Then the documents will be sorted in descending order by their weight and the documents that are not related to the user input topic will be removed form the list.
For the remaindng documents, it will group them by their topic and calcaluate the similarities between each of them using doc2vec model.</p>
<p>Finally, a csv file with six columns will be generated as well as the HTML verison of it. <br>
The first column will contain the path to article1. <br>
The second column will contain the path to article2 <br>
The third column will contain the topic (collection of keywords) of article1. <br>
The fourth column will contain the topic (collection of keywords) of article2. <br>
The fifth column will contain the path to the image file of article1 and the path to the image file of article2. <br>
The three column will contain the similarity between article1 and article2. <br></p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="id0">
<span class="sig-name descname"><span class="pre">findTopicForPapers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id0" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to the generated csv file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputTopic</strong> -- user input topic.</p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A path to the generated csv file and A path to the generated HTML file</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list[str]</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="endpoint-gettopic">
<h2>Endpoint: /getTopic<a class="headerlink" href="#endpoint-gettopic" title="Permalink to this headline">¶</a></h2>
<p>To find the topic of a paper you can use the /getTopic endpoint.</p>
<p><code class="docutils literal notranslate"><span class="pre">gettopic(userInputArticleLink):</span></code> function:</p>
<blockquote>
<div><p>For a given user input article link, this function will extract text from the file and build a lda model to get the topic of it.</p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="id1">
<span class="sig-name descname"><span class="pre">findTopicForPapers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id1" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to the generated csv file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputArticleLink</strong> -- </p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A path to LDA model</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="endpoint-getsimilarity">
<h2>Endpoint: /getsimilarity<a class="headerlink" href="#endpoint-getsimilarity" title="Permalink to this headline">¶</a></h2>
<p>To find the similarity between two papers you can use the /getsimilarity endpoint</p>
<p><code class="docutils literal notranslate"><span class="pre">getsimilarity(userInputArticleLink1,</span> <span class="pre">userInputArticleLink2):</span></code> function:</p>
<blockquote>
<div><p>For a given user input article links, this function will find the similarity between them.
It will first extract text from the file and build a doc2vec model using the text.
Then it will find the similarity between two documents using the function from doc2vec model.</p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="id2">
<span class="sig-name descname"><span class="pre">findTopicForPapers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id2" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to the generated csv file</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputArticleLink2</strong> (<em>userInputArticleLink1</em><em>,</em>) -- </p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str, str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>list of similarities</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>list[in]</p>
</dd>
</dl>
</dd></dl>

<p><code class="docutils literal notranslate"><span class="pre">getImagesFromFile(userInputArticleLink):</span></code> function:</p>
<blockquote>
<div><p>For a given user input article link, this function will extract images from the file and save it under img directory.</p>
</div></blockquote>
<dl class="py function">
<dt class="sig sig-object py" id="id3">
<span class="sig-name descname"><span class="pre">findTopicForPapers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">userInputTopic</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lang</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'en'</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#id3" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a path to image directory</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>userInputArticleLink</strong> -- </p>
</dd>
<dt class="field-even">Type</dt>
<dd class="field-even"><p>str</p>
</dd>
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>A path to image directory</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>str</p>
</dd>
</dl>
</dd></dl>

</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="#">word salad</a></h1>








<h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="#">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2022, Wenxiu Ye.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.4.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/index.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>