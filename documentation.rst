Clien's Concern:
================
With the ever evolving space that is climate change, there is a vast amount of data that is used across disciplines. 
Communicating climate change information has become increasingly more difficult as reliable baselines and equivalencies are scarce. 
NGOs, corporations and individuals should have the opportunity to have access to reliable climate change data and equivalencies in order to better understand this interconnective space. 
As the volume of technical and scientific publications increase across academia, industry, and governmental sources, the task of collecting relevant data on a particular topic of interest becomes increasingly onerous. 
Machine learning models leveraging natural language processing techniques offer an opportunity to extract meaningful information with semantic similarities rather than simple spelling. 
Visualizations of an embedded space of results could also be useful in determining outliers making spurious claims.

Project implementation:
=======================
To determine if the content of a document is reliable or not, one approach to it is to compare it with other documents with a similar topic. 
In order to accomplish this, we need to collect data from search engines and use NLP tools to process it. 
NLP(natural language processing) refers to a computer program that has the ability to understand text and spoken words in the same way as humans.
I have written a web scraper that will perform a serch on NASA’s website according to user input topic and download the documents in pdf format.
Once all the documents are downloaded, the code will convert them into a text file. To extract the topic from the texts, we can use a machine learnning model called LDA.
Latent Dirichlet Allocation (LDA) which is an unsupervised learning algorithm which finds the underlying topic of a document. 
The topic generated by LDA model is a collection of keywords. 
Just by looking at the keywords, you can identify what the topic is all about.
Before we can build the LDA model, we have to do some data cleaning. 
For example, we have to make all letters lowercase, remove punctuations, numbers, empty spaces and references if there’s any in the documents. 
We then split the document into sentences, sentences into words and then we lemmatize the words. 
Next we build the LDA model using python gensim library. 
Once the process is completed, we can get the underlying topic of the given documents. 
The topic of each document is a collection of keywords with weight. 
So the API will read all the keywords and try to find keywords that match the user’s input topic. 
Then we sort the documents in descending order by their weight.
The documents with weight zero will be discard as it proves not  to be related to what user is looking for.
For the remaindng documents, the code will group them by their topic.
Documents with similar topics will have similar content semantically. 
This can be useful info for the user to determine if the document is reliable or not.  
We calcaluate the similarities between each of them using doc2vec model. 
Doc2vec is an NLP tool for representing documents as a vector and is a generalizing of the word2vec method.


Usage
=====

Installation
------------

To use this API, please install python3 and the requied libraries.

.. code-block:: console

   (.venv) $ pip3 install -r requirements.txt



To run the API 
----------------

.. code-block:: console

   (.venv) $ cd api
   (.venv) $ python3 app.py


Endpoints for this API
=====================

Endpoint: /search
----------------

To download the files from NASA website, use the /search endpoint with a given topic. 
This endpoint will search the NASA's website based on the user's input topic and download all the files found.

``search(userInputTopic):`` function:

   This funciton will perform a search on NASA's website and all files found will be downloaded.
   Once all the files are downloaded, a csv file with three columns will be generated as well as a HTML verison of it; |br|
   the first column contain the link to the article where it is found. |br|
   the second column contain the link to the article description. |br|
   the third column contain the local path to the article. |br|
   
.. py:function:: search(userInputTopic)

   Return a path to the generated csv file 

   :param userInputTopic: user input topic.
   :type: str
   :return: A path to the generated csv file and a path to the generated HTML file
   :rtype: list[str]

``makeNewDiretoryForGivenTopic(userInputTopic):`` function:

   This function will create directories for user input topic to store the data.|br|
   If user types in climate change, then the main directory climatechange will be created along with six subdirectories;
   pdfclimatechange, paperclimatechange, txtclimatechange, csvclimatechange, imageclimatechange, ldaclimatechange |br|

   pdfclimatechange : used to store downloaded files |br|
   paperclimatechange : used to store the identified papers for given user input topic |br|
   txtclimatechange : used to store text extracted from papers |br|
   csvclimatechange : used to store the paths to datas |br|
   imageclimatechange : used to store the images extracted from the papers |br|
   ldaclimatechange : used to store the lda models for papers

.. py:function:: makeNewDiretoryForGivenTopic(userInputTopic)
   
   Return the directories names.(pdfDirName, paperDirName, imageDirName, csvDirName, txtDirName, ldaDirName)

   :param userInputTopic: user input topic.
   :type: str
   :return: The directory names for given user input topic
   :rtype: str, str, str, str, str, str


Endpoint: /extractInfoFromPapers
----------------------------

To extract the text and the images from the papers you can use the /extractInfoFromPapers endpoint

``extractInfoFromPapers(userInputTopic):`` function:

   This funciton will search for a directory name pdf{userInputTopic}(without spaces) and retrieve all the files stored there. 
   Once all the files are obtained; |br|
   -It will identify if the file is a paper or not by searching for abstract and refenereces in the article

   If the file is a paper, then it will extract text from it and save the text under the text{userInputTopic}(without spaces) directory.
   And it will extract images from it and save the images as one pdf file under the image{userInputTopic}(without spaces) directory. 

   Finally, a csv file with three columns will be generated as well as the HTML verison of it. |br|
   The first column will contain the path to the papers. |br|
   The second column will contain the path to the text file corresponding to the pappers. |br|
   The last column will contain the path to the image file. |br|

   In order to save this table, a CSV file will be generated.

.. py:function:: extractInfoFromPapers(userInputTopic, lang="en")

   Return a path to the generated csv file 

   :param userInputTopic: user input topic.
   :type: str
   :return: A path to the generated csv file and A path to the generated HTML file
   :rtype: list[str]

Endpoint: /findTopicForPapers
----------------------------

To find topic of all the pappers you can use the /findTopicForPapers endpoint

``findTopicForPapers(userInputTopic):`` function:

  This function will search for the csv file generated by the /extractInfoFromPapers endpoint. 
  It will retrieve 
  the path to all the papers, 
  the path to the text file, 
  the path to the images. Then it will go through all the text file individually and find the underlying topic of the text using LDA topic model. |br|
  The topic of each document is a collection of keywords with weight. Once the topic for each topic is identified, 
  it will read all the keywords and try to find keywords that match the user’s input topic. 
  If there is a match, then we keep the paper and classify it as document that related to what the user is looking for. 
  Then the documents will be sorted in descending order by their weight.
  
  Finally, a csv file with six columns will be generated as well as the HTML verison of it. |br|
  The first column will contain the path to the papers. |br|
  The second column will contain the path to the text file corresponding to the papers. |br|
  The third column will contain the LDA topic key wordlist. |br|
  The fourth column will contain the path to the LDA model. |br|
  The fifth column will contain weight. |br|
  The sixth column will contain the path to the image file. |br|

.. py:function:: findTopicForPapers(userInputTopic)

   Return a path to the generated csv file 

   :param userInputTopic: user input topic.
   :type: str
   :return: A path to the generated csv file and A path to the generated HTML file
   :rtype: list[str]

Endpoint: /findSimilarityForPapers
----------------------------

To find topic of all the pappers you can use the /findTopicForPapers endpoint

``findSimilarityForPapers(userInputTopic):`` function:

  This function will search for the csv file generated by the /extractInfoFromPapers endpoint. 
  It will retrieve the path to all the papers, the path to the text file, the path to the images. 
  Then it will go through all the text file individually and find the underlying topic of the text using LDA topic model. 
  The topic of each document is a collection of keywords with weight. Once the topic for each topic is identified, 
  it will read all the keywords and try to find keywords that match the user’s input topic. 
  If there is a match, then we keep the paper and classify it as document that related to what the user is looking for. 
  Then the documents will be sorted in descending order by their weight and the documents that are not related to the user input topic will be removed form the list. 
  For the remaindng documents, it will group them by their topic and calcaluate the similarities between each of them using doc2vec model. 

  Finally, a csv file with six columns will be generated as well as the HTML verison of it. |br|
  The first column will contain the path to article1. |br|
  The second column will contain the path to article2 |br|
  The third column will contain the topic (collection of keywords) of article1. |br|
  The fourth column will contain the topic (collection of keywords) of article2. |br|
  The fifth column will contain the path to the image file of article1 and the path to the image file of article2. |br|
  The three column will contain the similarity between article1 and article2. |br|

.. py:function:: findTopicForPapers(userInputTopic)

   Return a path to the generated csv file 

   :param userInputTopic: user input topic.
   :type: str
   :return: A path to the generated csv file and A path to the generated HTML file
   :rtype: list[str]


Endpoint: /getTopic
----------------------------

To find the topic of a paper you can use the /getTopic endpoint.


``gettopic(userInputArticleLink):`` function:
   
   For a given user input article link, this function will extract text from the file and build a lda model to get the topic of it.

.. py:function:: findTopicForPapers(userInputTopic, lang="en")

   Return a path to the generated csv file 

   :param userInputArticleLink:
   :type: str
   :return: A path to LDA model
   :rtype: str


Endpoint: /getsimilarity
----------------------------

To find the similarity between two papers you can use the /getsimilarity endpoint


``getsimilarity(userInputArticleLink1, userInputArticleLink2):`` function:

   For a given user input article links, this function will find the similarity between them. 
   It will first extract text from the file and build a doc2vec model using the text.
   Then it will find the similarity between two documents using the function from doc2vec model.

.. py:function:: findTopicForPapers(userInputTopic, lang="en")

   Return a path to the generated csv file 

   :param userInputArticleLink1, userInputArticleLink2:
   :type: str, str
   :return: list of similarities
   :rtype: list[in]


``getImagesFromFile(userInputArticleLink):`` function:
   
   For a given user input article link, this function will extract images from the file and save it under img directory.

.. py:function:: findTopicForPapers(userInputTopic, lang="en")

   Return a path to image directory

   :param userInputArticleLink:
   :type: str
   :return: A path to image directory
   :rtype: str

.. |br| raw:: html

      <br>
